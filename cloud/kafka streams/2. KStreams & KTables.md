## KStreams
- All `inserts` (append)
- similar to a log, infinite, unbounded
- when reading from a topic that's not compacted, or data is partial/transactional

## KTables
- All `upserts` on non null values
- deletes on null values
- similar to a table
- paralle with log compacted topics
- when reading from a topic that's log-compacted (aggregations), if you need a structure, every update is self sufficient

### MapValues / Map
- MapValues (preferred): does not change keys, does not trigger a repartition, for both KStreams and KTables
- Map: affects both keys and values, triggers repartition, for `KStreams` only

### Filter / FilterNot
does not change keys/values, does not trigger repartition, for both KStreams and KTables

### FlatMapValues / FlatMap
- flatMapValues: does not change keys, no repartition, for `KStreams` only
- flatMap: change keys, trigger repartition, for `KStreams` only

### KStream branch
split KStream by predicates, if no matches, records are dropped
```
KStream<String, Long>[] branches = stream.branch(
    (key, value) -> value > 100,
    (key, value) -> value > 10,
    (key, value) -> value > 10
```

### KStream selectKey
assign a new key == for repartition. better know exactly where the partitioning happens
```
rekeyed = stream.selectKey((key, value) -> key.substring(0, 1)
```

### Reading from Kafka
```
// KStream, KTable, GlobalKTable
KStream<String, String> textLines = builder.stream(
    Serdes.String(),  // optional
    Serdes.Long(), // optional
    "word-count-input-topic");
```


### Writing to Kafka
```
// write to a topic
wordCounts.toStream().to("word-count-output", Produced.with(Serdes.String(), Serdes.Long()));

// write to a topic and get stream/table from a topic
// KStream<String, Long> newStream = stream.through("user-click-topic");
```

### Log compaction
- improve performance when dealing KTables, less reads, has to manually enable
- only delete a few messages
- ensures log contains the last know value for a key, useful if only requires snapshot (not history)
- order remains; offsets are skipped
- any consumer reading from head can still see all messages (default 24hrs)
- doesn't prevent from pushing duplicate data, neither from reading duplicated data
- may fail sometimes, need enough memory
```
--config cleanup.policy=compact \
--config min.cleanable.dirty.ratio=0.005 \
--config segment.ms=10000
```

### KStream & KTable Duality
- Stream as Table
- Table as Stream

KTable to KStream
```
KStream<byte[], String> stream = table.toStream();
```

KStream to KTable
```
// chain a groupByKey() and an aggregation step (count, aggregate, reduce)
KTable<String, Long> table = usersAndColors.groupByKey().count();

// or write back to Kafka
stream.to("intermediary-topic")
KTable<String, String> table = build.table("intermediary-topic")
```

