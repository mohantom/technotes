Kafka basics
-------------------------
- 2020.06.01
- Udemy: Apache Kafka Series - Learn Apache Kafka for Beginners v2 (2018)

- [Code1](https://courses.datacumulus.com/downloads/kafka-beginners-bu5/)
- [Code2](https://github.com/simplesteph/kafka-beginners-course)
- [kafka-stack-docker-compose](https://github.com/simplesteph/kafka-stack-docker-compose)

## Topics
    - a stream of data, similar to a table, no limit

## Partitions
    - topics are split in partitions, each is ordered;
    - each message gets an incremental id, called offset (infinite)
    - offset is for a specific parition only
    - data is immutable, kept for a week by default
    - data is assigned to a partition unless a key is provided

## Cluster
    - multiple brokers (bootstrap server)
    - broker has ID (int), each has some topic partitions
    - to start is 3 brokers, 
    - replication 2 or 3
    - Only one broker can be a leader for a given partition, others are ISR (in-sync replica)
    
## Producers
[configure producer](https://kafka.apache.org/documentation/#producerconfigs)
    - writes data to topics
    - automatically knows which broker and partition to write to
    - acks
        - 0: producer won't wait for ack (possible data loss)
        - 1: wait for leader ack (limited data loss)
        - all: leader + replica ack (no data loss)
    - If key is set, all messages for that key will go to the same partition; If you need ordering for a field (ex, truck_id)
    - default retries = 2147483647 for Kafka > 2.1; delivery.timeout.ms = 120000ms
    - retry may cause messages to be sent out of order. 
        - idempotent producers: won't introduce duplicates on network error.
        - keep order even if process in parallel: max.in.flight.requests=5
        - `producerProps.put("enable.idempotence", true)` (producer level), `min.insync.replicas=2` (broker/topic level)
        - These two implies acks=all, retries=MAX_INT, max.in.flight.requests.per.connection=5 (for v>1.0), with order guarantee

## Message compression
`compression.type`=none (default), gzip, lz4, snappy (recommended)
[benchmar](https://blog.cloudflare.com/squeezing-the-firehose)
- `linger.ms`: ms before producer waits to send out, so Kafka can send out in batch; if bath is full before this, it will be sent right away
- `batch.size` (max bytes, default=16KB)
- producerPros.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
- producerPros.put(ProducerConfig.LINGER_MS_CONFIG, "20");
- producerPros.put(ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32 * 1024)); // 32KB

## Consumers
[configure consumers](https://kafka.apache.org/documentation/#consumerconfigs)
    - read data from a topic, knows which broker to read from
    - data is read in order
    - consumer offsets: stores the offsets a consumer has been reading, committed automatically live in a topic __consumer_offsets
    - delivery semantics: consumer can choose when to commit offsets:
        - `at most once` (message may be lost), do not use
        - `at least once` (may duplicate)
        - `exactly once`: only possible kafka => kafka using Kafka Streams; for DB, use `idempotent` consumer

- Consumer group
    - each consumer in a group read from exclusive partitions
    - If more consumers than partitions, some consumers will be inactive

- Zookeeper
    - operates with odd number of servers (3, 5, 7)
    - leader, follower
    - does not store offsets


## Install and run on Windows
1. Download and Setup Java 8 JDK
2. Download the Kafka binaries from https://kafka.apache.org/downloads
3. Extract Kafka at the root of C:\
4. Setup Kafka bins in the Environment variables section by editing Path
5. Try Kafka commands using kafka-topics.bat (for example)
6. Edit Zookeeper & Kafka configs using NotePad++ https://notepad-plus-plus.org/download/
    - zookeeper.properties: dataDir=C:/kafka_2.12-2.0.0/data/zookeeper (yes the slashes are inversed)
    - server.properties: log.dirs=C:/kafka_2.12-2.0.0/data/kafka (yes the slashes are inversed)
7. Start Zookeeper in one command line: zookeeper-server-start.bat config\zookeeper.properties
8. Start Kafka in another command line: kafka-server-start.bat config\server.properties


## Conduktor: Kafka UI
[Conduktor](https://www.conduktor.io)
[KafkaCat](https://github.com/edenhill/kafkacat) is an open-source alternative to using the Kafka CLI, created by Magnus Edenhill.


## Consumer seek and assign
```
TopicPartition partionToReadFrom = new TopicPartition(topic, 0);
long offsetToReadFrom = 15L;
consumer.assign(Arrays.asList(partitionToReadFrom));

consumer seek(partitionToReadFrom, offsetToReadFrom);
```

## Elasticsearch on cloud
[Elasticsearch](https://bonsai.io)
```
IndexRequest indexRequest = new IndexRequest("fruits")
  .id(fruitCount.getId())   // to prevent duplicate processing
  .source(fruitJson, XContentType.JSON);
```

## Manual commits offsets
not enable auto-commits, and synchronously process batches. 
default is enabled, if async, you may lose data

```
consumerPros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

while(true) {
    batch += consumer.poll(Duration.ofMillis(100));
    if (isReady(batch)) {
        doSomethingSynch(bath);
        consumer.commitSync();
    }
}
```

## Replay
reset-offsets
```
kafka-consumer-groups --bootstrap-server 127.0.0.1:9002 --group kafka-demo-es --reset-offsets --execute --to-earliest --topic twitter_tweets
```
ES will not have duplicates if same messages are replayed

## Schema registry
to handle bad data, field name change, data format change.
This needs to installed separately

[Which Kafka API should I use?](https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e)


## Partition count, replication factor
- each partition can handle a few MB/s
- more partition -> more parallelism and more throughput
- But more elections for Zookeeper, more files opened on Kafka
- 2 x broker (< 6 brokers), 1 x broker ( > 6 brokers)

- replicas, > 2, usually 3, max 4 (higher latency, more data on disk)
- never set to 1 for production
- one broker < 2000~4000 partitions; cluster < 20,000 partitions => more clusters

[Topics Naming Convention](https://medium.com/@criccomini/how-to-paint-a-bike-shed-kafka-topic-naming-conventions-1b7259790073)

## Case study
- MovieFlix: play position, recommendation, analytics
- GetTaxi: user position, taxi position, surge price calculation
- CQRS (MySocialMedia): user post pic/text, others can like/comments
    - Command Query Responsibility Segregation (CQRS)
- MyBank: real-time banking, alert with large transactions
- Big data ingestion: 
- Logging & metrics aggregation

## Kafka enterprise for admin
- monitoring & operations: scale, add/remove brokers, zero downtime,...
- security (hard to set up...)
- multi cluster & MirrorMaker: works well in a single region -> multiple clusters with some replication
    - replication does not preserve offsets, just data
[Mirror Maker](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330)
[Mirror Maker Performance Tuning](https://engineering.salesforce.com/mirrormaker-performance-tuning-63afaed12c21?gi=af0d1d20dbf4)
[Confluent Relicator](https://docs.confluent.io/current/multi-dc-deployments/replicator/replicator-tuning.html#improving-network-utilization-of-a-connect-task)
[Netflix talk on Kafka](https://www.confluent.io/kafka-summit-sf17/multitenant-multicluster-and-hieracrchical-kafka-messaging-service/)
[Uber UReplicator](https://eng.uber.com/ureplicator-apache-kafka-replicator)


## Advance configuration

## Annex

