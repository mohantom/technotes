Kafka Streams
--------------------------
- 2020.05.27
- Udemy: Apache Kafka Series - Kafka Streams for Data Processing (2017)
- [Streams Develper Guide](https://docs.confluent.io/current/streams/developer-guide/index.html#streams-developer-guide-dsl-joins)

1. Standard Java app
2. No need for separate cluster
3. Highly scalable, elastic, fault tolerant
4. Exactly once
5. One record at a time (no batch)

If need to write the result to an external system, it's not recommended to do inside Kafka Streams,
instead use Kafka Connect API

[Kafka Streams Demo Application](https://docs.confluent.io/current/streams/kafka-streams-examples/docs/index.html)
[confluentinc examples](https://github.com/confluentinc/examples)
[bbejeck kafka-streams](https://github.com/bbejeck/kafka-streams)
[gwenshap kafka-examples](https://github.com/gwenshap/kafka-examples)
[gwenshap kafka-streams-stockstats](https://github.com/gwenshap/kafka-streams-stockstats)
[Kafka Streams API](https://kafka.apache.org/0110/documentation/streams)


## Kafka streams vs Spark streaming, NiFi, Flink
- Micro batch vs per data streaming
- No additional cluster required
- Scales easily by adding java processes
- Exactly once (at least once for Spark streaming)
- NiFi is drap-n-drop
- [Kafka & Spark Streaming](https://www.quora.com/What-are-the-differences-and-similarities-between-Kafka-and-Spark-Streaming)

[Course code](https://courses.datacumulus.com/downloads/kafka-streams-sn2)


## Word Counter
- Stream
- Stream processor
- Topology
    - Source processor: take data directly from Kafka topic, does not transform data
    - Sink processor: no children, sends data directly to Kafka topic
 
### Configuration, can be done in code
- bootstrap.servers: need to connect to Kafka
- auto.offset.reset.config: set to 'earliest' to consume from start
- application.id: specific to Streams app, used for
    - Consumer group.id = application.id
    - default client.id prefix
    - prefix to internal changelog topics
- default.[key|value].serde: for serialization and deserilization of data

data in Kafka streams is <key, value>


### Topology
1. `Stream` from Kafka
2. `MapValues` lowercase
3. `FlatMapValues` split by space
4. `SelectKey` to apply a key
5. `GroupByKey` before aggregation
6. `Count` occurences in each group
7. `To` in order to write the results back to Kafka

// shutdown hook
```
Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
```


### intermediary topics
Kafka streams may eventually create internal intermediary topics

- repartition topics: when tranforming the key of streams
- changelog topics: when aggregations, Kafka streams save compacted data in the topics
- prefixed with app id
- user should not touch them 


### Scale up
start same number of apps as partitions


## Exactly once
- producers are no idempotent: if sent twice, Kafka make sure only keep one copy
- write multiple messages as one transaction
```
Properties = props = new Properties();
props.put(StreamsConfig.PROCESSING_GURARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
KafkaStreams streams = new KafkaStreams(builder, props)
```

## Summary Diagram
![Summary Diagram](../data/streams-stateful_operations.png)

### TODO
0. Kafka on docker
1. MovieProcessApp: Kafka consumer, scan folders, convert to video object, publish to Kafka
2. ES7 app subscribe to Kafka: dump to ES
3. Kafka Streams does calculations?, dump to ES. Or endpoint for aggregations
4. UI to show stats

