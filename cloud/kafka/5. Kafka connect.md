# Kafka Connect
- Udemy: Apache Kafka Series - Kafka Connect Hands-on Learning (2017)
twitter -> ES & PostgreSql
- [Source code](https://courses.datacumulus.com/downloads/kafka-connect-09a)

Does not cover Avro

## Pros & Cons
- Pros
    - Simple to set up, just configurations
    - Can add source on the fly

- Cons
    - Need separate cluster
    - Only connect, no processing. Will need Kafka Streams, Spark, Storm or others to process data
    - Security?

## Concept
- source => kafka  (Kafka Connect Source)
- kafka => kafka  (Kafka Streams)
- kafka => sink  (Kafka Connect Sink)
- kafka => app

In addition to Kafka cluster, need Connect cluster

- multiple loaded connectors, re-usable code (jar)
- many connectors are open sourced
- connectors + user config => Tasks
- A job config may spawn multiple tasks
- tasks executed by connect workers (standalone or cluster)

### how to find logs
    - standalone: terminal
    - cluster: localhost:3030/logs, `connect-distributed.log`

update landoop for kafka-connect, [landoop](https://github.com/landoop/fast-data-dev)
`docker pull landoop/fast-data-dev`

## Example: FileStreamSourceConnector
read a file and load content to Kafka
ref: `kafka-connect-tutorial-sources.sh`
every line is sent as a message
if connect restarts, file will not be processed twice

// task


### Standalone mode
```shell script
docker run --rm -it -v %cd%:/tutorial --net=host landoop/fast-data-dev:cp3.3.0 bash

# we launch the kafka connector in standalone mode:
cd /tutorial/source/demo-1
# create the topic we write to with 3 partitions
kafka-topics --create --topic demo-1-standalone --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
# Usage is connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties]
connect-standalone worker.properties file-stream-demo-standalone.properties
# write some data to the demo-file.txt !
```

also need `worker.properties` file

// `file-stream-demo-standalone.properties`
```properties
# These are standard kafka connect parameters, need for ALL connectors
name=file-stream-demo-standalone
connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector
tasks.max=1
# Parameters can be found here: https://github.com/apache/kafka/blob/trunk/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java
file=demo-file.txt
topic=demo-1-standalone
```

### Distributed mode

```shell script
# B) FileStreamSourceConnector in distributed mode:
# create the topic we're going to write to
docker run --rm -it --net=host landoop/fast-data-dev:cp3.3.0 bash
kafka-topics --create --topic demo-2-distributed --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
# you can now close the new shell

# head over to 127.0.0.1:3030 -> Connect UI
# Create a new connector -> File Source
# Paste the configuration at source/demo-2/file-stream-demo-distributed.properties

# Now that the configuration is launched, we need to create the file demo-file.txt
docker ps
# ssh into kafka-connect container
docker exec -it <containerId> bash
touch demo-file.txt
echo "hi" >> demo-file.txt
echo "hello" >> demo-file.txt
echo "from the other side" >> demo-file.txt

# Read the topic data
docker run --rm -it --net=host landoop/fast-data-dev:cp3.3.0 bash
kafka-console-consumer --topic demo-2-distributed --from-beginning --bootstrap-server 127.0.0.1:9092
# observe we now have json as an output, even though the input was text!

```

// We don't need the worker.properties in cluster mode
[Add connector from UI](http://localhost:3030/kafka-connect-ui/#/cluster/fast-data-dev)
Paste the following to UI
```properties
# These are standard kafka connect parameters, need for ALL connectors
name=file-stream-demo-distributed
connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector
tasks.max=1
# Parameters can be found here: https://github.com/apache/kafka/blob/trunk/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java
file=demo-file.txt
topic=demo-2-distributed

# Added configuration for the distributed mode:
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
```

## Available connectors
[Confluent](https://www.confluent.io/hub)
[Landoop](https://github.com/landoop/fast-data-dev#enable-additional-connectors)


## TwitterSourceConnector
```shell script
docker run --rm -it --net=host landoop/fast-data-dev:cp3.3.0 bash
kafka-topics --create --topic demo-3-twitter --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181
# Start a console consumer on that topic
kafka-console-consumer --topic demo-3-twitter --bootstrap-server 127.0.0.1:9092
```

// add this from connect UI localhost:3030
```properties
# Basic configuration for our connector
name=source-twitter-distributed
connector.class=com.eneco.trading.kafka.connect.twitter.TwitterSourceConnector
tasks.max=1
topic=demo-3-twitter
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true

# Twitter connector specific configuration
twitter.consumerkey=9jT7rYU09ELMSibxgVK7tRpRs
twitter.consumersecret=ggQg21pZLFcexZ0PjTX1zRtG0bHHuWFhMx4DxGfRAVD2xwrHR4
twitter.token=2932095118-245Itf5j2H01MDrRq0MMPNtPHTIxER4e3Cj2hoD
twitter.secret=7kAn2qXA7wJJ8k46ySj1lri9Reey9h2NLAxLPtFUkjVgb
track.terms=programming,java,kafka,scala
language=en
```

## Elasticsearch connector
```properties
# Basic configuration for our connector
name=sink-elastic-twitter-distributed
connector.class=io.confluent.connect.elasticsearch.ElasticsearchSinkConnector

# We can have parallelism here so we have two tasks!
tasks.max=2
topics=demo-3-twitter

# the input topic has a schema, so we enable schemas conversion here too
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true

# ElasticSearch connector specific configuration
# # http://docs.confluent.io/3.3.0/connect/connect-elasticsearch/docs/configuration_options.html
connection.url=http://elasticsearch:9200
type.name=kafka-connect

# because our keys from the twitter feed are null, we have key.ignore=true
key.ignore=true

# some (dummy) settings we need to add to ensure the configuration is accepted
# The bug is tracked at https://github.com/Landoop/fast-data-dev/issues/42
topic.index.map="demo-3-twitter:index1"
topic.key.ignore=true
topic.schema.ignore=true
```

## Rest API
[docs](http://docs.confluent.io/3.2.0/connect/managing.html#common-rest-examples)
```shell script
docker run --rm -it --net=host landoop/fast-data-dev:cp3.3.0 bash
# Install jq to pretty print json
apk update && apk add jq

# 1) Get Worker information
curl -s 127.0.0.1:8083/ | jq
# 2) List Connectors available on a Worker
curl -s 127.0.0.1:8083/connector-plugins | jq
# 3) Ask about Active Connectors
curl -s 127.0.0.1:8083/connectors | jq
# 4) Get information about a Connector Tasks and Config
curl -s 127.0.0.1:8083/connectors/source-twitter-distributed/tasks | jq
# 5) Get Connector Status
curl -s 127.0.0.1:8083/connectors/file-stream-demo-distributed/status | jq
# 6) Pause / Resume a Connector (no response if the call is succesful)
curl -s -X PUT 127.0.0.1:8083/connectors/file-stream-demo-distributed/pause
curl -s -X PUT 127.0.0.1:8083/connectors/file-stream-demo-distributed/resume
# 7) Get Connector Configuration
curl -s 127.0.0.1:8083/connectors/file-stream-demo-distributed | jq
# 8) Delete our Connector
curl -s -X DELETE 127.0.0.1:8083/connectors/file-stream-demo-distributed
# 9) Create a new Connector
curl -s -X POST -H "Content-Type: application/json" --data '{"name": "file-stream-demo-distributed", "config":{"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector","key.converter.schemas.enable":"true","file":"demo-file.txt","tasks.max":"1","value.converter.schemas.enable":"true","name":"file-stream-demo-distributed","topic":"demo-2-distributed","value.converter":"org.apache.kafka.connect.json.JsonConverter","key.converter":"org.apache.kafka.connect.json.JsonConverter"}}' http://127.0.0.1:8083/connectors | jq
# 10) Update Connector configuration
curl -s -X PUT -H "Content-Type: application/json" --data '{"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector","key.converter.schemas.enable":"true","file":"demo-file.txt","tasks.max":"2","value.converter.schemas.enable":"true","name":"file-stream-demo-distributed","topic":"demo-2-distributed","value.converter":"org.apache.kafka.connect.json.JsonConverter","key.converter":"org.apache.kafka.connect.json.JsonConverter"}' 127.0.0.1:8083/connectors/file-stream-demo-distributed/config | jq

```

## PostgreSql connector


## Custom connector


